{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import plotly.offline as py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/03 15:21:13 WARN Utils: Your hostname, MacBook-Air-di-Teodoro.local resolves to a loopback address: 127.0.0.1; using 192.168.135.184 instead (on interface en0)\n",
      "23/01/03 15:21:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/03 15:21:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/01/03 15:21:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"flights\").getOrCreate()\n",
    "import json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType, TimestampType\n",
    "with open(\"../../util/schema.json\",\"r\") as f:\n",
    "    schema = StructType.fromJson(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"../../data.nosync/cleaned/cleaned_flights.csv\",schema=schema, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column 'label' that is 1 if the flight is delayed and 0 if it is not\n",
    "df = df.withColumn(\"label\", when(df[\"ArrDelay\"] > 0, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Year',\n",
       " 'Quarter',\n",
       " 'Month',\n",
       " 'DayofMonth',\n",
       " 'DayOfWeek',\n",
       " 'FlightDate',\n",
       " 'Reporting_Airline',\n",
       " 'Tail_Number',\n",
       " 'Flight_Number_Reporting_Airline',\n",
       " 'Origin',\n",
       " 'Dest',\n",
       " 'CRSDepTime',\n",
       " 'DepTime',\n",
       " 'DepDelay',\n",
       " 'DepDelayMinutes',\n",
       " 'DepDel15',\n",
       " 'DepartureDelayGroups',\n",
       " 'DepTimeBlk',\n",
       " 'TaxiOut',\n",
       " 'WheelsOff',\n",
       " 'WheelsOn',\n",
       " 'TaxiIn',\n",
       " 'CRSArrTime',\n",
       " 'ArrTime',\n",
       " 'ArrDelay',\n",
       " 'ArrDelayMinutes',\n",
       " 'ArrDel15',\n",
       " 'ArrivalDelayGroups',\n",
       " 'ArrTimeBlk',\n",
       " 'Cancelled',\n",
       " 'Diverted',\n",
       " 'CRSElapsedTime',\n",
       " 'ActualElapsedTime',\n",
       " 'AirTime',\n",
       " 'Distance',\n",
       " 'DistanceGroup',\n",
       " 'DivAirportLandings',\n",
       " 'ORIGIN_STATE',\n",
       " 'ORIGIN_LATITUDE',\n",
       " 'ORIGIN_LONGITUDE',\n",
       " 'DEST_STATE',\n",
       " 'DEST_LATITUDE',\n",
       " 'DEST_LONGITUDE',\n",
       " 'label']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    " 'Quarter',\n",
    " 'Month',\n",
    " 'DayofMonth',\n",
    " 'DayOfWeek',\n",
    " 'Reporting_Airline',\n",
    " 'Origin',\n",
    " 'Dest',\n",
    " #'DepDelay',\n",
    " 'CRSDepTime',\n",
    " 'CRSArrTime',\n",
    " 'CRSElapsedTime',\n",
    " 'AirTime',\n",
    " 'Distance',\n",
    " 'ORIGIN_STATE',\n",
    " 'DEST_STATE',\n",
    " 'label'\n",
    " ]\n",
    "\n",
    "\n",
    "# mantain only the features in features list\n",
    "df = df.select(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(False,0.07,seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "437379"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Â get the number of rows in the dataframe\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Quarter', 'int'),\n",
       " ('Month', 'int'),\n",
       " ('DayofMonth', 'int'),\n",
       " ('DayOfWeek', 'int'),\n",
       " ('Reporting_Airline', 'string'),\n",
       " ('Origin', 'string'),\n",
       " ('Dest', 'string'),\n",
       " ('CRSDepTime', 'int'),\n",
       " ('CRSArrTime', 'int'),\n",
       " ('CRSElapsedTime', 'double'),\n",
       " ('AirTime', 'double'),\n",
       " ('Distance', 'double'),\n",
       " ('ORIGIN_STATE', 'string'),\n",
       " ('DEST_STATE', 'string'),\n",
       " ('label', 'int')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "categoricalCols = [field for (field, dataType) in df.dtypes\n",
    "if dataType == \"string\"]\n",
    "\n",
    "indexOutputCols = [x + \"Index\" for x in categoricalCols]\n",
    "oheOutputCols = [x + \"OHE\" for x in categoricalCols]\n",
    "\n",
    "stringIndexer = StringIndexer(inputCols=categoricalCols,\n",
    "                                outputCols=indexOutputCols,\n",
    "                                handleInvalid=\"skip\")\n",
    "oheEncoder = OneHotEncoder(inputCols=indexOutputCols,\n",
    "                            outputCols=oheOutputCols)\n",
    "\n",
    "numericCols = [field for (field, dataType) in df.dtypes\n",
    "                    if ((dataType == \"double\" or dataType == \"int\" ) & (field != \"label\"))]\n",
    "\n",
    "assemblerInputs = oheOutputCols + numericCols\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=assemblerInputs,\n",
    "                        outputCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# create the pipeline\n",
    "pipeline = Pipeline(stages=[stringIndexer, oheEncoder, vecAssembler])\n",
    "\n",
    "# fit the pipeline to the data\n",
    "pipelineModel = pipeline.fit(df)\n",
    "\n",
    "# transform the data\n",
    "df_proc = pipelineModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the features and label columns\n",
    "df_proc = df_proc.select(\"features\",\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/03 14:58:15 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(758,[8,98,328,66...|    1|\n",
      "|(758,[8,98,328,66...|    0|\n",
      "|(758,[8,25,375,65...|    1|\n",
      "|(758,[8,51,338,64...|    0|\n",
      "|(758,[8,51,338,64...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the first 5 rows\n",
    "df_proc.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of rows with label 1\n",
    "df_proc.filter(df_proc.label == 1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of rows with label 0\n",
    "df_proc.filter(df_proc.label == 0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6678037681605175"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_rate = 2501251/3745488\n",
    "sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test\n",
    "train, test = df_proc.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_1 = train.filter(train.label == 1)\n",
    "\n",
    "\n",
    "train_0 = train.filter(train.label == 0).sample(False, sample_rate, seed=42)\n",
    "\n",
    "\n",
    "# merge the two datasets\n",
    "train = train_1.union(train_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.orderBy(rand())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "140519"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the number of rows with label 1\n",
    "train.filter(train.label == 1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "140354"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the number of rows with label 0\n",
    "train.filter(train.label == 0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a grid search to find the best hyperparameters\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", seed=42)\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.maxDepth, [6,15]) \\\n",
    "    .addGrid(rf.maxBins, [60,100]) \\\n",
    "    .addGrid(rf.numTrees, [60, 120]) \\\n",
    "    .build()\n",
    "\n",
    "# use the grid with 8-fold cross validation\n",
    "crossval = CrossValidator(estimator=rf,\n",
    "                            estimatorParamMaps=paramGrid,\n",
    "                            evaluator=MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\"),\n",
    "                            numFolds=5)\n",
    "\n",
    "# fit the model\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# get the best model\n",
    "bestModel = cvModel.bestModel\n",
    "\n",
    "# get the best hyperparameters\n",
    "bestModel.extractParamMap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â create the model\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=30,maxDepth=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "model = rf.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "predictions = model.transform(train)\n",
    "\n",
    "# evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))\n",
    "\n",
    "# print the accuracy\n",
    "print(\"Accuracy = %g \" % accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Error = 0.419311 \n",
    "Accuracy = 0.580689 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the predictions and the true label\n",
    "predictions.select(\"prediction\", \"label\").show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
