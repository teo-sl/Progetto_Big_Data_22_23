{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.offline as py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/26 18:16:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"flights\").getOrCreate()\n",
    "import json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType, TimestampType\n",
    "with open(\"../matrix/schema.json\",\"r\") as f:\n",
    "    schema = StructType.fromJson(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"../../data.nosync/cleaned/cleaned_flights.csv\",schema=schema, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# get the unique Origin and Dest airports\n",
    "origin_airports = df.select(df.Origin).distinct()\n",
    "dest_airports = df.select(df.Dest).distinct()\n",
    "\n",
    "# merge the two dataframes\n",
    "airports = origin_airports.union(dest_airports).distinct().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to list\n",
    "airports = airports['Origin'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning:\n",
      "\n",
      "Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the differente flights dates\n",
    "dates = df.select(df.FlightDate).distinct().toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FlightDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>2013-12-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>2013-12-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>2013-12-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>2013-12-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>2013-12-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>365 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    FlightDate\n",
       "0   2013-01-01\n",
       "1   2013-01-02\n",
       "2   2013-01-03\n",
       "3   2013-01-04\n",
       "4   2013-01-05\n",
       "..         ...\n",
       "360 2013-12-27\n",
       "361 2013-12-28\n",
       "362 2013-12-29\n",
       "363 2013-12-30\n",
       "364 2013-12-31\n",
       "\n",
       "[365 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort the dates\n",
    "dates = dates.sort_values(by=['FlightDate']).reset_index(drop=True)\n",
    "dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_sp = spark.read.csv(\"../../preprocessing/airports.csv\", header=True,inferSchema=True)\n",
    "airports = airports_sp.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get corrispondence between us states abbreviations and full names, from a dataset on internet, puerto rico included\n",
    "states = pd.read_csv(\"../../states.csv\",delimiter=\"\\t\",header=None)\n",
    "states.columns = [\"State\",\"unk\",\"Abbreviation\"]\n",
    "states.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def routes_queries(df,date_start,date_end,origin=\"BOS\",query=\"NumFlights\",scope=\"airports\"):\n",
    "    df_aggregated = df.filter((col(\"Origin\") == origin))\n",
    "    df_aggregated = df_aggregated.filter((col(\"FlightDate\") >= date_start) & (col(\"FlightDate\") <= date_end))\n",
    "    # aggregate by flight date, day of weew, Origin and Dest, count the number of flights and average the arrival delay\n",
    "    if scope == \"airports\":\n",
    "        df_aggregated = df_aggregated.groupBy(\"Origin\",\"Dest\",\"ORIGIN_LATITUDE\",\"ORIGIN_LONGITUDE\",\"DEST_LATITUDE\",\"DEST_LONGITUDE\").agg({\"ArrDelay\": \"avg\",\"*\":\"count\"}).withColumnRenamed(\"avg(ArrDelay)\", \"AverageArrivalDelay\").withColumnRenamed(\"count(1)\", \"NumFlights\")\n",
    "    else:\n",
    "        df_aggregated = df_aggregated.groupBy(\"ORIGIN_STATE\",\"DEST_STATE\").agg({\"ArrDelay\": \"avg\",\"*\":\"count\",\"ORIGIN_LATITUDE\":\"avg\",\"DEST_LATITUDE\":\"avg\",\"ORIGIN_LONGITUDE\":\"avg\",\"DEST_LONGITUDE\":\"avg\"}).withColumnRenamed(\"avg(ArrDelay)\", \"AverageArrivalDelay\").withColumnRenamed(\"count(1)\", \"NumFlights\")\n",
    "        # rename columns\n",
    "        df_aggregated = df_aggregated.withColumnRenamed(\"avg(ORIGIN_LATITUDE)\", \"ORIGIN_LATITUDE\").withColumnRenamed(\"avg(ORIGIN_LONGITUDE)\", \"ORIGIN_LONGITUDE\").withColumnRenamed(\"avg(DEST_LATITUDE)\", \"DEST_LATITUDE\").withColumnRenamed(\"avg(DEST_LONGITUDE)\", \"DEST_LONGITUDE\")\n",
    "\n",
    "    # sort by query and take the first 100 rows\n",
    "    df_aggregated = df_aggregated.orderBy(df_aggregated[query].desc()).limit(100)\n",
    "    return df_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a timestamp object from the string\n",
    "date_start = \"2013-02-01\"\n",
    "date_end = \"2013-12-28\"\n",
    "# convert the string to a timestamp object\n",
    "date_start = pd.Timestamp(date_start)\n",
    "date_end = pd.Timestamp(date_end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aggregatedp = routes_queries(df,date_start,date_end)\n",
    "#show on map the routes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aggregatedp=df_aggregatedp.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the airports dataframe with the aggregated dataframe compare with Origin and Dest\n",
    "df_aggregatedp = df_aggregatedp.merge(airports, left_on=\"Origin\", right_on=\"IATA\")\n",
    "df_aggregatedp = df_aggregatedp.merge(airports, left_on=\"Dest\", right_on=\"IATA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aggregated = df_aggregatedp.sort_values(by=\"AverageArrivalDelay\", ascending=False).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aggregated[\"AverageArrivalDelay\"] = df_aggregated[\"AverageArrivalDelay\"]+ df_aggregated[\"AverageArrivalDelay\"].min()*-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aggregated[\"AverageArrivalDelay\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_routes(df,date_start,date_to,origin=\"BOS\",query=\"NumFlights\",scope=\"airports\"):\n",
    "    df_aggregated=routes_queries(df,date_start,date_to,origin,query,scope).toPandas()\n",
    "    print(len(df_aggregated))\n",
    "    if scope == \"airports\":\n",
    "        df_aggregated = df_aggregated.merge(airports, left_on=\"Origin\", right_on=\"IATA\")\n",
    "        df_aggregated = df_aggregated.merge(airports, left_on=\"Dest\", right_on=\"IATA\")\n",
    "    else:\n",
    "        # join with states\n",
    "        df_aggregated = df_aggregated.merge(states, left_on=\"ORIGIN_STATE\", right_on=\"Abbreviation\")\n",
    "        df_aggregated = df_aggregated.merge(states, left_on=\"DEST_STATE\", right_on=\"Abbreviation\")\n",
    "        \n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    source_to_dest = zip(df_aggregated[\"ORIGIN_LATITUDE\"], df_aggregated[\"DEST_LATITUDE\"],\n",
    "                         df_aggregated[\"ORIGIN_LONGITUDE\"], df_aggregated[\"DEST_LONGITUDE\"],\n",
    "                         df_aggregated[query])\n",
    "\n",
    "    ## Loop thorugh each flight entry to add line between source and destination\n",
    "    for slat,dlat, slon, dlon, num_flights in source_to_dest:\n",
    "        fig.add_trace(go.Scattergeo(\n",
    "                            lat = [slat,dlat],\n",
    "                            lon = [slon, dlon],\n",
    "                            mode = 'lines',\n",
    "                            line = dict(width = 1, color=\"red\"),\n",
    "                            # disable hover info\n",
    "                            hoverinfo=\"skip\",\n",
    "                            textposition=\"top center\"\n",
    "                    ))\n",
    "\n",
    "    ## Logic to create labels of source and destination cities of flights\n",
    "    if scope==\"airports\":\n",
    "        cities = df_aggregated[\"AIRPORT_x\"].values.tolist()+df_aggregated[\"AIRPORT_y\"].values.tolist()\n",
    "    else:\n",
    "        cities = df_aggregated[\"State_x\"].values.tolist()+df_aggregated[\"State_y\"].values.tolist()\n",
    "\n",
    "    scatter_hover_data = [city for city in cities]\n",
    "\n",
    "    if query == \"AverageArrivalDelay\":\n",
    "        df_aggregated[query] = df_aggregated[query] + df_aggregated[query].min()*-1\n",
    "\n",
    "    # create a column as concatenation of AIRPORT_x and query\n",
    "    target_col = \"AIRPORT_y\" if scope==\"airports\" else \"State_y\"\n",
    "\n",
    "    df_aggregated[target_col] = df_aggregated[target_col] + \"<br>\"+query+\" : \"+ df_aggregated[query].astype(str)\n",
    "    text = df_aggregated[target_col].values.tolist()\n",
    "\n",
    "    df_aggregated[query]=df_aggregated[query]/df_aggregated[query].max()\n",
    "    ## Loop thorugh each flight entry to plot source and destination as points.\n",
    "    fig.add_trace(\n",
    "        go.Scattergeo(\n",
    "                    lon = df_aggregated[\"DEST_LONGITUDE\"].values.tolist(),\n",
    "                    lat = df_aggregated[\"DEST_LATITUDE\"].values.tolist(),\n",
    "                    hoverinfo = 'text',\n",
    "                    text = text,\n",
    "                    mode = 'markers',\n",
    "                    marker = dict(size = df_aggregated[query]*20+1, color = 'blue', opacity=0.9)),\n",
    "                    # define the size of the marker based on the number of flights\n",
    "                    #     \n",
    "        )\n",
    "\n",
    "    ## Update graph layout to improve graph styling.\n",
    "    fig.update_layout(title_text=\"Connection Map Depicting Flights from Brazil to All Other Countries\",\n",
    "                      height=700, width=900,\n",
    "                      margin={\"t\":0,\"b\":0,\"l\":0, \"r\":0, \"pad\":0},\n",
    "                      showlegend=False,\n",
    "                      geo= dict(showland = True, landcolor = 'white', countrycolor = 'grey', bgcolor=\"lightgrey\",scope='north america'))\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_routes(df,date_start,date_end,\"BOS\",\"NumFlights\",\"airports\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def states_map_query(df,group):\n",
    "    df_aggregated = df.groupBy(group).agg({\"ArrDelay\": \"avg\", \"*\":\"count\"}).withColumnRenamed(\"avg(ArrDelay)\", \"ArrDelay\")\n",
    "    df_aggregated = df_aggregated.withColumnRenamed(\"count(1)\",\"count\")\n",
    "    return df_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = states_map_query(df,\"DEST_STATE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â get the row with the maximum average delay\n",
    "tmp.sort(\"ArrDelay\",ascending=False).show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_states_map(df,group,query):\n",
    "    df_avg = states_map_query(df,group).toPandas()\n",
    "    df_avg = df_avg.drop(df_avg[df_avg[group] == \"AS\"].index)\n",
    "    df_avg = df_avg.drop(df_avg[df_avg[group] == \"GU\"].index)\n",
    "    \n",
    "    fig = px.choropleth(locations=df_avg[group], locationmode=\"USA-states\", color=df_avg[query], scope=\"usa\")\n",
    "\n",
    "    # add title \"Average delay by state origin\"\n",
    "    fig.update_layout(title_text=f\"{group} : {query}\")\n",
    "    \n",
    "    return fig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_states_map(df,\"ORIGIN_STATE\",\"ArrDelay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by dest state and make the avg of the arrival delay\n",
    "df_avg = states_map_query(df,\"DEST_STATE\").toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order by the average delay\n",
    "df_avg.sort_values(\"ArrDelay\",ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by dest state and origin state and make the avg of the arrival delay\n",
    "df_avg = states_map_query(df,[\"DEST_STATE\",\"ORIGIN_STATE\"]).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg.sort_values(\"ArrDelay\",ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def origin_dest_query(df,query=\"ArrDelay\"):\n",
    "    # filter the dataframe using timestamp from_date and to_date    \n",
    "    if query==\"count\":\n",
    "        df = df.groupBy(\"ORIGIN_STATE\",\"DEST_STATE\").agg({\"*\": \"count\"}).withColumnRenamed(\"count(1)\", \"count\")\n",
    "    else:\n",
    "        df = df.groupBy(\"ORIGIN_STATE\",\"DEST_STATE\").agg({\"ArrDelay\": \"avg\"}).withColumnRenamed(\"avg(ArrDelay)\", \"ArrDelay\")\n",
    "\n",
    "    # crate a new column with the origin and destination\n",
    "    # order by query, descendant order\n",
    "    df = df.orderBy(df[query].desc())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg = origin_dest_query(df,\"ArrDelay\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg.sort_values(\"ArrDelay\",ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates(df):\n",
    "    dates = df.select(\"FlightDate\").distinct().orderBy(\"FlightDate\", ascending=True).toPandas()[\"FlightDate\"]\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = get_dates(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def origin_dest_query(df,from_date,to_date,query=\"ArrDelay\"):\n",
    "    # filter the dataframe using timestamp from_date and to_date\n",
    "    df = df.filter(df[\"FlightDate\"].between(from_date,to_date))\n",
    "    \n",
    "    if query==\"count\":\n",
    "        df = df.groupBy(\"ORIGIN_STATE\",\"DEST_STATE\").agg({\"*\": \"count\"}).withColumnRenamed(\"count(1)\", \"count\")\n",
    "    else:\n",
    "        df = df.groupBy(\"ORIGIN_STATE\",\"DEST_STATE\").agg({\"ArrDelay\": \"avg\"}).withColumnRenamed(\"avg(ArrDelay)\", \"ArrDelay\")\n",
    "\n",
    "    # crate a new column with the origin and destination\n",
    "    # order by query, descendant order\n",
    "    df = df.orderBy(df[query].desc())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def origin_dest_plot(df,from_date,to_date,query=\"ArrDelay\"):\n",
    "    df_pd = origin_dest_query(df,from_date,to_date,query).toPandas()\n",
    "    # make a join over STATE_ORIGIN and Abbreviation in states dataframe, rename the columns\n",
    "    df_pd = df_pd.merge(states, left_on=\"ORIGIN_STATE\", right_on=\"Abbreviation\").\\\n",
    "        rename(columns={\"State\": \"Origin\"}).\\\n",
    "        merge(states, left_on=\"DEST_STATE\", right_on=\"Abbreviation\").\\\n",
    "        rename(columns={\"State\": \"Dest\"})\n",
    "    # create a new column with the origin and destination\n",
    "    df_pd[\"Origin-Dest\"] = df_pd[\"Origin\"] + \" - \" + df_pd[\"Dest\"]\n",
    "    fig = px.pie(df_pd.head(20), values=query, names='Origin-Dest', title=f'{query} by Origin-Dest')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates[len(dates)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_dest_plot(df,dates[0],dates[len(dates)-1],\"ArrDelay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def taxi_time_query(df,orig_dest):\n",
    "    # group by orig_dest\n",
    "    df = df.groupBy(orig_dest,\"FlightDate\").agg({\"TaxiIn\": \"avg\", \"TaxiOut\": \"avg\",\"*\":\"count\"}).\\\n",
    "        withColumnRenamed(\"avg(TaxiIn)\", \"TaxiIn\").\\\n",
    "        withColumnRenamed(\"avg(TaxiOut)\", \"TaxiOut\").\\\n",
    "        withColumnRenamed(\"count(1)\", \"count\")\n",
    "    # order by TaxiIn, descendant order\n",
    "    df = df.orderBy(df[\"TaxiIn\"].desc())\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Year',\n",
    " 'Quarter',\n",
    " 'Month',\n",
    " 'DayofMonth',\n",
    " 'DayOfWeek',\n",
    " 'Reporting_Airline',\n",
    " 'Origin',\n",
    " 'Dest',\n",
    " 'CRSDepTime',\n",
    " 'CRSArrTime',\n",
    " 'ArrDel15',\n",
    " 'DistanceGroup',\n",
    " 'ORIGIN_STATE',\n",
    " 'DEST_STATE',]\n",
    "\n",
    "\n",
    "# mantain only the features in features list\n",
    "df = df.select(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the column types in df_ml\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert arr del 15 to int\n",
    "df= df.withColumn(\"ArrDel15\", df[\"ArrDel15\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample the 50% of the dataframe\n",
    "df = df.sample(False,0.5,seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "categorical= ['Reporting_Airline','Origin','Dest','ORIGIN_STATE','DEST_STATE']\n",
    "\n",
    "stringindexer_stages = [StringIndexer(inputCol=c, outputCol='strindexed_' + c) for c in categorical]\n",
    "stringindexer_stages += [StringIndexer(inputCol='ArrDel15', outputCol='label')]\n",
    "\n",
    "onehotencoder_stages = [OneHotEncoder(inputCol='strindexed_' + c, outputCol='onehot_' + c) for c in categorical]\n",
    "\n",
    "feature_columns = ['onehot_' + c for c in categorical]\n",
    "vectorassembler_stage = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
    "\n",
    "all_stages = stringindexer_stages + onehotencoder_stages + [vectorassembler_stage]\n",
    "pipeline = Pipeline(stages=all_stages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_columns = feature_columns + ['features', 'label']\n",
    "cuse_df = pipeline_model.transform(df).\\\n",
    "            select(final_columns)\n",
    "            \n",
    "cuse_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, test = cuse_df.randomSplit([0.7, 0.3], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(featuresCol='features', labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model on training data\n",
    "dt_model = dt.fit(training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on test data\n",
    "predictions = dt_model.transform(test)\n",
    "\n",
    "# evaluate the model using accuracy\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %g\" % (accuracy))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "dt_model.save(\"dt_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import DecisionTreeClassificationModel\n",
    "from pyspark.ml.classification import DecisionTreeClassificationModel\n",
    "# load the model\n",
    "dt_model = DecisionTreeClassificationModel.load(\"dt_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on test data\n",
    "predictions = dt_model.transform(test)\n",
    "\n",
    "# evaluate the model using accuracy\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %g\" % (accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â get the confusion matrix\n",
    "predictions.groupBy(\"label\",\"prediction\").count().show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
